AWS SageMaker Inference
---
AWS SageMaker推理模块的调研

首页主打的Feature介绍
---
传送门：https://aws.amazon.com/sagemaker/deploy/

- 模型监控
    - 目的：监控由于训练集和真实数据间的分布偏差造成的模型表现的退化
    - 特点
        - 支持built-in和custom报警规则
        - 支持监控常见的issue: 异常值，数据分布的drift，预测准确率的变化等
        - 可以在SageMaker Studio里使用，监控数据和AWS CloudWatch打通
        - 报警的"可视化"：支持查看哪些数据造成了模型表现的变化
        - 推理请求和响应的持久化
    - 详细介绍<br/>
    
        作为SageMaker Deploy页面上的第一个feature出现，可见AWS对模型监控feature的重视，也侧面反映了在
        推理这个领域，"监控模型"是算法工程师最为关注的一个方面。
        
        除了支持自定义报警规则算是监控报警领域的标配外，其他的几个小feature都是非常有亮点的。
        
        首先是支持监控常见issue，例如异常值，数据分布drift等，在监控告警的背景下融入了机器学习的具体场景。
        使用者通常希望监控系统除了能够完成检测和报告异常的基本功能外，还能够指出造成异常的原因，甚至是给出
        异常的分析报告。常见的做法就是像SageMaker这样针对特定业务领域下高发的异常做针对性的场景监控。
        
        其次是监控报警和Studio的打通，算法工程师可以直接在Studio内查看可视化的监控界面。纵观SageMaker的
        各种特性，基本都可以在Studio（Notebook）内以代码方式实现，实现了产品的一体化和闭环。帮助提升用户
        黏性。
        
        最后是报警的"可视化"及推理请求和响应的持久化。算法工程师可以直接查看模型在哪些样本上的表现最差，从
        而将这些"异常值"加入训练集或者对模型做出针对性的调整。推理请求和响应持久化也方便了算法工程师对模型
        的线上推理历史做回溯分析。总的来说，其目的都是一个：帮助算法工程师更方便地分析问题，迭代模型。
       
- 一键部署
    - 目的：尽可能减少算法工程师进行"模型->服务"这一转化的成本
    - 特点
        - "一键部署"（实际上并不能一键部署，但是整体操作还是比较简单的）
        - 多地域部署
        - 自动扩缩容
        - 弹性推理（GPU加速）
    - 详细介绍<br/>
    
        模型部署是在线推理模块最基本的功能，但算法工程师通常不想在部署上花费太多的时间和精力。所以该模块的基
        本目标就是尽可能的将部署上的工程细节隐藏，或者说托管。
        
        虽然没有做到真正意义上的"一键部署"，但是SageMaker的总体部署流程还是比较简便的。在创建模型时，指定好
        模型的存储路径，模型的运行时镜像，容器的DNS域名，环境变量及网络安全配置。然后创建一个HTTP Endpoint，
        在EndPoint内设定好使用的模型，计算资源，弹性推理和实例数等，就完成了一次模型到在线服务的转化。
        
        自动扩缩容算是"模型部署"下比较有亮点的feature，用户可以选择使用built-in的metrics类型，或者自定义
        metrics类型。模块还提供自动扩缩容历史的查询以及负载测试等功能。
        
        弹性推理实际上就是使用GPU进行推理加速，其特点在于不强制使用整张GPU，而是提供fractional的GPU进行加速。
        另一个特点在于用户可以在自己的Notebook实例（开发测试机）上使用不同类型的弹性推理加速器，用于评估最合适
        的加速器。
        
         可以在Studio内纯代码实现。
        
- 批量推理
    - 目的：解决非实时场景下的批量推理任务
    - 特点
        - 相对于直接使用线上服务进行离线批量推理，这个feature本身就是其特点
    - 详细介绍<br/>
        
        对于一些对实时性要求不高的业务场景，部署在线推理服务的成本是比较大的，大多数时间下服务都处于空闲状态，
        浪费了资源。如果复用现有的在线服务，又可能造成资源的抢占，影响了本来就需要实时响应的业务。
        
        SageMaker的批量推理功能实际上就是一个单次执行的任务。用户指定要使用的模型，数据集，计算资源及其配置，
        输入输出格式，推理的存放路径等，运行任务后，SageMaker会自动拉起所选计算资源进行计算，将计算结果到处
        到给定存放路径，待完成计算任务后再销毁计算资源。
        
        可以在Studio内纯代码实现。
        
- 模型编译器和运行时
    - 目的：解决模型跨平台部署会遇到的工程和性能问题
    - 特点
        - 支持和优化了主流机器学习框架：Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, and ONNX
        - 支持和优化了主流操作系统：Android, Linux, and Windows
        - 支持和优化了主要的硬件平台：Ambarella, ARM, Intel, Nvidia, NXP, Qualcomm, Texas Instruments, and Xilinx
        - 支持部署到边缘侧
    - 详细介绍<br/>
    
        SageMaker Neo官方宣称是使用了神经网络对模型及框架进行了优化，实际效果未知。
        
        SageMaker Neo包含两部分：模型编译器和模型运行时。其工作过程是首先通过模型编译器将模型编译成二进制文
        件，然后在运行时内运行二进制文件即可。
        
        SageMaker Neo在Console里有专门的compilation job一栏，打开后选择要编译的模型，输入数据格式，框架，
        数据输出地址及设备即可完成一次模型编译。
    
- 与K8s集成（公有云ai平台的feature，暂略）

- 推理流水线
    - 目的：支持数据前、后处理，特征工程，串行多个模型进行推理等需求，复用数据处理代码
    - 特点
        - 可复用的数据处理，特征工程代码
    - 详细介绍<br/>

        机器学习模型的训练通常都包含大量的前后数据处理，特征工程等工序。SageMaker推理流水线支持算法工程师复用
        训练阶段的代码，降低工作量。
        
        另外，实际中算法工程师可能会串行多个模型进行推理，推理流水线支持该需求。请求以HTTP形式在模型链中传输，
        性能未知。
        
- 多模型部署
    - 目的：将大量模型部署到同一个容器内以节省资源，降低部署工作量
    - 特点
        - "一键部署"多个模型
        - 支持A/B实验
    - 评论<br/>
    
        从SageMaker官方文档上对"多模型部署"的介绍得知，该功能是为了满足针对单个用户训练模型的场景，例如根据每
        个music app用户的历史听歌数据进行模型训练，以达到更精准的音乐推送效果。
        
        该功能的基本实现思路如下，Endpoint作为前端的API接受层，会将收到的请求根据一定规则路由到后面的某个实例
        内。多模型部署模式下，各模型共享实例内存。由于模型数量可能较大，容器采用懒加载模型的方式。并且当某个实例
        内已加载了该模型时，请求会优先路由到该实例下。当实例内存占用率较高时，将采用一定策略清除部分模型。

- 机器学习推理专用芯片
    - 目的：采用专为推理设计的芯片进行推理加速
    - 特点
        - 硬件层面的加速
    - 评论<br/>
    
        专门为机器学习推理设计的芯片，效果未知。
