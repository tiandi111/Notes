Neural Networks and Deep Learning
---
Chapter 3 | Training Deep Neural Networks
---

##3.2 Backpropagation


##3.5 Gradient-Descent Strategy

- Learning Rate Decay
    - Exponential Decay
    - Inverse Decay
- Momentum
    - Normal Momentum
        - Accelerate learning
        - avoid local optimum
        - avoid gradient slowdown (flat region)
    - Nesterov Momentum
        - incorporate lookahead information
- Parameter-Specific Learning Rate
    - AdaGrad
    