Distributed Training
---

Two distributed styles
---
- Running parallel experiments over many GPUs(and servers) to search for good hyperparameters
- Distributing the training of a single network over may GPUs(and servers), reduce training time

Model parallelism versus data parallelism
---
- Model parallelism(in-graph replication):

    some neural networks models are so large they cannot fit in memory of a single device(GPU).
    Such model need to be split over many devices, carrying out the training in parallel on the
    devices. For example, different layers in a network may be trained in parallel on the devices.
    
    Use same data for each device.
    
- Data parallelism(between-graph replication):

    Use the same model for every device, but train the model in each device using different 
    training samples. Because each device trains on different samples, it computes different 
    changes to the model(the "gradients"). However, the algorithm depends on using the combined
    results of all processing for each new iteration, as if the algorithm ran on a single processor.
    Therefore, each device has to send all of its changes to all of the models at all the other
    devices.
    
Synchronous versus asynchronous distributed training
---

Stochastic gradient descent(SGD) is and iterative algorithm for finding optimal values, it involves
multiple rounds of training, where the results of each round are incorporated into the model prepared
for the next round. 

The rounds can be either synchronously or asynchronously.

- Synchronous distributed training: 
    - Howï¼šeach device train their local model using different part of asingle batch. After all devices
     finished training, the gradients are collected to update the model.Then the updated model is sent
     back to each device.
    - Overhead factor: 
        - large model
        - slow network
        - slow device(straggler)
        - total number of iteration, batch size and accuracy
            - small number of iteration -> large batch size -> low accuracy
            
- Asynchronous distributed training:
    - How: no device waits for updates to the model from any other device. There can be two ways to 
    achieve asynchronous training. One is P2P architecture in which each device runs a loop that reads
    data, computes the gradients, sends them to all devices and updates the model to the latest version.
    In centralized architecture, the devices send their output in the form of gradients to the parameter 
    server which collect and aggregate gradients.
    It is different from synchronous training in that devices immediately download the new model after 
    sending their gradient.

Parameter server architecture
---

Ring-allreduce architecture
---

Distributed Tensorflow
---
Because workers calculate gradients they are typically placed on GPUs; parameter servers only aggregate
gradient and broadcast updates, so they are typically placed on CPUs.

One of the worker, the chief worker, coordinates model training, initialized the model, counts the number
of training steps completed, monitors the session, saves logs for tensorBoard, and saves and restores model 
checkpoint to recover from failures. The chief worker also manages failures, ensuring fault tolerance if a
worker or parameter server fails. 